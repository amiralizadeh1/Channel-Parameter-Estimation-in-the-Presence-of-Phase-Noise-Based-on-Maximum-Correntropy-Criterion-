#Author: Amir Alizadeh. Please contact me and let me know of your comments on the code at a.alizadeh@mail.um.ac.ir
#noise adder only adds additive noise. ie it return RR instead of R. change it experimenting with tikhonof PN.
import numpy as np
np.random.seed(7)
import matplotlib.pyplot as plt
import scipy
import winsound
LB = 16000
snrdb = 60
num_taps = 5
learning_rate = 0.01
kappa = 4 #smaller kappa indicated more severe noise
sigma2 = 2
coeff = np.concatenate(([ (1+1j)], np.zeros(1), [ 1+1j], np.zeros(1), [1]))/np.sqrt(5) #make it random later #make it case study later
coeff_new = np.concatenate(([ (1-1j)], np.zeros(1), [ 1-1j], np.zeros(1), [1]))/np.sqrt(6) 


## adaptive aglorithm with Second Order Statistics
## inputs:: QAM: input training data, R:target training data, num_taps: length of filter to be estimated, learning_rate: stepsiz
## outputs:: error history of the learning algorithm, weights: estimated filter weights, D: MSD history of the learning algorith
def LMS_MMSE(QAM, R, R_new, num_taps, LR): 
    num_points = len(QAM)
    W = np.zeros(num_taps)
    E = np.zeros(num_points)
    D = []
    for n in range(num_taps, num_points):
        
        XX = QAM[n-num_taps:n]
        X = np.flip(XX)
        
        
        if n<int(len(QAM)/2): E[n]= R[n-1] - np.dot(X, W)
        else: E[n]= R_new[n-1] - np.dot(X, W)
        
        
        W = W + (LR*X.conj()*E[n])
        if  n<int(len(QAM)/2): D.append(np.mean(abs(coeff - W)**2))
        else:  D.append(np.mean(abs(coeff_new - W)**2))
    return E,W,D

## adaptive aglorithm with Higher Order Statistics
## inputs:: QAM: input training data, R:target training data, num_taps: length of filter to be estimated, learning_rate: stepsiz
##          SIGMA2: kernel bandwidth
## outputs:: error history of the learning algorithm, weights: estimated filter weights, D: MSD history of the learning algorith

def LMS_MCC(QAM, R, R_new, num_taps, learning_rate,SIGMA2):  #adaptive algorithm with Higher Order Statistics
    num_points = len(QAM)
    W = np.zeros(num_taps)
    E = np.zeros(num_points)
    D = []
    for n in range(num_taps, num_points):
        
        XX = QAM[n-num_taps:n]
        X = np.flip(XX)
        
        if  n<int(len(QAM)/2): E[n]= R[n-1] - np.dot(X, W)
        else: E[n]= R_new[n-1] - np.dot(X, W)
        
        first = learning_rate/SIGMA2
        second = np.exp(-(E[n] * np.conj(E[n]))/(2*SIGMA2))
        
        W = W + first*second*E[n]*X.conj()
        #weights = weights +( (first*second*E[n]*x.conj()) /sum(abs(x) ** 2))
        if  n<int(len(QAM)/2): D.append(np.mean(abs(coeff - W)**2))
        else:  D.append(np.mean(abs(coeff_new - W)**2))
    return E,W,D



##  moving average function, used for averaging MSD
def moving_average(DATA, WINDOW):
    return np.convolve(DATA, np.ones(WINDOW), 'valid') / WINDOW

## Serial to parralel function to map bits into 4-bit groups
def SP(bits):
    LS = (LB/4)
    return bits.reshape(int(LS), 4)

## mapping table of 16-QAM modulation
mapping_table = {
    (0,0,0,0) : -3-3j,
    (0,0,0,1) : -3-1j,
    (0,0,1,0) : -3+3j,
    (0,0,1,1) : -3+1j,
    (0,1,0,0) : -1-3j,
    (0,1,0,1) : -1-1j,
    (0,1,1,0) : -1+3j,
    (0,1,1,1) : -1+1j,
    (1,0,0,0) :  3-3j,
    (1,0,0,1) :  3-1j,
    (1,0,1,0) :  3+3j,
    (1,0,1,1) :  3+1j,
    (1,1,0,0) :  1-3j,
    (1,1,0,1) :  1-1j,
    (1,1,1,0) :  1+3j,
    (1,1,1,1) :  1+1j
}

## maps 4-bit groups into QAM symbols
def Mapping(bits):
    return np.array([mapping_table[tuple(b)] for b in bits])

def weiner(N,var):
    Wei = np.zeros(N+1)
    Wei[0] = np.sqrt(var/2) * (np.random.randn())
    for n in range(N):
        Wei[n+1] = Wei[n] + np.sqrt(var/2) * (np.random.randn())
    Wei = Wei[1:]
    
    return Wei

def noise_adder(Convolved,SNRdb,Kappa):
    Signal_power_rx = np.mean(abs(Convolved)**2)
    Var_noise = Signal_power_rx * 10**(-SNRdb/10)  # calculate noise power based on signal power and SNR
    print(f'Var_noise is {Var_noise}')
    print(f'Signal_power_rx is {Signal_power_rx}')
    print(f'SNRdb is {SNRdb}')
    # Generate complex noise with given variance
    N = np.sqrt(Var_noise/2) * (np.random.randn(*Convolved.shape)+1j*np.random.randn(*Convolved.shape))
    PHI = np.random.vonmises(0, kappa, len(Convolved))
    
    w = weiner(len(Convolved),0.1)
    
    RR = Convolved + N
    R = np.multiply(RR,np.exp(PHI * 1j))

    return RR # because this is the PN free version, no need to return R.



for b3 in [0, 1]:
    for b2 in [0, 1]:
        for b1 in [0, 1]:
            for b0 in [0, 1]:
                B = (b3, b2, b1, b0)
                Q = mapping_table[B]
                plt.plot(Q.real, Q.imag, 'bo') 
                plt.text(Q.real, Q.imag+0.2, "".join(str(x) for x in B), ha='center')

demapping_table = {v : k for k, v in mapping_table.items()}
bits = np.random.binomial(n=1, p=0.5, size= LB)


bits_sp = SP(bits)
qam = Mapping(bits_sp)



convolved = np.convolve(qam, coeff)
convolved_new = np.convolve(qam, coeff_new)


#channel_energy = sum(abs(coeff)**2)
#signal_power_tx = np.mean(abs(qam)**2)
#signal_power_rx = np.mean(abs(convolved)**2)  

    
#print(f'TX signal power is {signal_power_tx}, RX signal power is {signal_power_rx}, noise power is {var_noise}')
#print(f'channel energy is {channel_energy}')
    
# Generate complex noise with given variance
#n = np.sqrt(var_noise/2) * (np.random.randn(*convolved.shape)+1j*np.random.randn(*convolved.shape))
#phi = np.random.vonmises(0, kappa, len(convolved))


r = noise_adder(convolved,snrdb,kappa)
r_new = noise_adder(convolved_new,snrdb,kappa)
###############################################################################################
e_lms,w_lms,deviation_lms = LMS_MMSE(qam, r, r_new, num_taps, learning_rate)
e_mcc,w_mcc,deviation_mcc = LMS_MCC(qam, r, r_new, num_taps, learning_rate,sigma2)

plt.figure(figsize=(14,6))
#plt.subplot(1,1,1)
plt.plot(10*np.log10(e_lms),'b',10*np.log10(e_mcc),'r')
plt.legend(["e_lms","e_mcc"],loc = "upper right")
#   plt.xlim([3500,4000])
plt.show()



print(f'weights obtained by lms_mmse algorithm:')
print(w_lms)
print(f'weights obtained by lms_mcc algorithm:')
print(w_mcc)

plt.figure(figsize=(10,6))
plt.plot(10*np.log10(deviation_lms),'b',10*np.log10(deviation_mcc),'r--')
plt.legend(["MSE for mse-lms","MSE for mcc-lms"],loc = "upper right")
#   plt.xlim([3500,4000])

plt.xlabel("Iterations")
plt.ylabel("MSE(dB)")
plt.savefig('mse for LMS.jpg',dpi=300,bbox_inches='tight')
plt.show()

#10*np.log(deviation_mcc[int(LB/8):-1])

print(f'MCC_mean at kappa={kappa} and snr={snrdb} and KBW={sigma2} is {np.mean(10*np.log10(deviation_mcc[int(LB/8+LB/16):-1]))}')
print(f'MCC-LMS at kappa={kappa} is {np.mean(10*np.log10(deviation_lms[int(LB/8+LB/16):-1])) - np.mean(10*np.log10(deviation_mcc[int(LB/8+LB/16):-1]))}')

deviation_lms_avg = moving_average(deviation_lms,500)
deviation_mcc_avg = moving_average(deviation_mcc,500)


print(f'snrdB is {snrdb}')

winsound.Beep(1000,300)
